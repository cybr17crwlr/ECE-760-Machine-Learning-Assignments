{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hingeLoss(x, y):\n",
    "    return torch.max(torch.zeros_like(y), 1-y*x).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "X, Y = data.data, data.target\n",
    "X = preprocessing.normalize(X)\n",
    "print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=100, random_state=2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=100, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(torch.nn.Module):\n",
    "    def __init__(self, X, kernel='linear', gamma=1.0, gammaGrad=True, p=2):\n",
    "        super().__init__()\n",
    "        assert kernel in ['linear', 'rbf', 'poly']\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        \n",
    "        if kernel == 'linear':\n",
    "            self._kernel = self.linear\n",
    "            self._num_c = self.X.shape[1]\n",
    "        \n",
    "        elif kernel == 'rbf':\n",
    "            self._kernel = self.rbf\n",
    "            self._num_c = self.X.shape[0]\n",
    "            self._gamma = torch.nn.Parameter(torch.FloatTensor([gamma]),\n",
    "                                             requires_grad=gammaGrad)\n",
    "        \n",
    "        elif kernel == 'poly':\n",
    "            self._p=p\n",
    "            self._kernel = self.poly\n",
    "            self._num_c = self.X.shape[0]\n",
    "           \n",
    "        else:\n",
    "            assert False\n",
    "            \n",
    "        self._w = torch.nn.Linear(in_features=self._num_c, out_features=1)\n",
    "\n",
    "    def rbf(self, x):\n",
    "        y = self.X.repeat(x.size(0), 1, 1)\n",
    "        return torch.exp(-self._gamma*((x[:,None]-y)**2).sum(dim=2))\n",
    "\n",
    "    def poly(self, x, c=1):\n",
    "        y = self.X.repeat(x.size(0), 1, 1)\n",
    "        return ((x@(self.X.T))+c) ** self._p\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear(x):\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self._kernel(x)\n",
    "        y = self._w(y)\n",
    "        return y\n",
    "    \n",
    "    def train(self, x, y, X_val, y_val, epochs=200, lambda_reg=0):\n",
    "        x = torch.FloatTensor(x)\n",
    "        y = 2*y-1\n",
    "        y = torch.FloatTensor(y)\n",
    "        X_val = torch.FloatTensor(X_val)\n",
    "        y_val = 2*y_val-1\n",
    "        valat=epochs/10\n",
    "        optim=torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "        for i in range(epochs):\n",
    "            optim.zero_grad()\n",
    "            pred=self(x)\n",
    "            loss=hingeLoss(pred,y.unsqueeze(1))+lambda_reg*torch.norm(self._w.weight,1)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            ypred=self(X_val).detach()\n",
    "            ypred=np.sign(ypred.transpose(1,0).numpy().reshape(y_val.shape))\n",
    "            if i%valat==0:\n",
    "                print(\"Validation Accuracy at epoch\",i,\":\",accuracy_score(y_val, ypred))\n",
    "        print(\"Training Loss is :\",loss.item())\n",
    "    \n",
    "    def test(self, x_test, y_test):\n",
    "        x_test = torch.FloatTensor(x_test)\n",
    "        y_test = 2*y_test-1\n",
    "        ypred=self(x_test).detach()\n",
    "        ypred=np.sign(ypred.transpose(1,0).numpy().reshape(y_test.shape))\n",
    "        print(\"Test Accuracy:\", accuracy_score(y_test,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy at epoch 0 : 0.56\n",
      "Validation Accuracy at epoch 100 : 0.56\n",
      "Validation Accuracy at epoch 200 : 0.56\n",
      "Validation Accuracy at epoch 300 : 0.56\n",
      "Validation Accuracy at epoch 400 : 0.56\n",
      "Validation Accuracy at epoch 500 : 0.56\n",
      "Validation Accuracy at epoch 600 : 0.56\n",
      "Validation Accuracy at epoch 700 : 0.56\n",
      "Validation Accuracy at epoch 800 : 0.56\n",
      "Validation Accuracy at epoch 900 : 0.56\n",
      "Training Loss is : 0.6992371678352356\n",
      "Test Accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "linSVM=SVM(X, kernel='linear')\n",
    "linSVM.train(X_train, y_train, X_val, y_val, epochs=1000)\n",
    "linSVM.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy at epoch 0 : 0.44\n",
      "Validation Accuracy at epoch 1000 : 0.44\n",
      "Validation Accuracy at epoch 2000 : 0.56\n",
      "Validation Accuracy at epoch 3000 : 0.64\n",
      "Validation Accuracy at epoch 4000 : 0.85\n",
      "Validation Accuracy at epoch 5000 : 0.69\n",
      "Validation Accuracy at epoch 6000 : 0.69\n",
      "Validation Accuracy at epoch 7000 : 0.71\n",
      "Validation Accuracy at epoch 8000 : 0.74\n",
      "Validation Accuracy at epoch 9000 : 0.79\n",
      "Training Loss is : 72.92414855957031\n",
      "Test Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "polySVM=SVM(X_train, kernel='poly', p=5)\n",
    "polySVM.train(X_train, y_train, X_val, y_val, epochs=10000)\n",
    "polySVM.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy at epoch 0 : 0.56\n",
      "Validation Accuracy at epoch 1000 : 0.63\n",
      "Validation Accuracy at epoch 2000 : 0.88\n",
      "Validation Accuracy at epoch 3000 : 0.91\n",
      "Validation Accuracy at epoch 4000 : 0.9\n",
      "Validation Accuracy at epoch 5000 : 0.88\n",
      "Validation Accuracy at epoch 6000 : 0.88\n",
      "Validation Accuracy at epoch 7000 : 0.89\n",
      "Validation Accuracy at epoch 8000 : 0.89\n",
      "Validation Accuracy at epoch 9000 : 0.9\n",
      "Training Loss is : 0.23805198073387146\n",
      "Test Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "rbfSVM=SVM(X_train, kernel='rbf')\n",
    "rbfSVM.train(X_train , y_train, X_val, y_val, epochs=10000)\n",
    "rbfSVM.test(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(torch.nn.Module):\n",
    "    def __init__(self, X,kernel='linear', gamma=1.0, gammaGrad=True, p=2):\n",
    "        super().__init__()\n",
    "        assert kernel in ['linear', 'rbf', 'poly']\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        \n",
    "        if kernel == 'linear':\n",
    "            self._kernel = self.linear\n",
    "            self._num_c = self.X.shape[1]\n",
    "\n",
    "        elif kernel == 'rbf':\n",
    "            self._kernel = self.rbf\n",
    "            self._num_c = self.X.shape[0]\n",
    "            self._gamma = torch.nn.Parameter(torch.FloatTensor([gamma]),\n",
    "                                             requires_grad=gammaGrad)\n",
    "        \n",
    "        elif kernel == 'poly':\n",
    "            self._p=p\n",
    "            self._kernel = self.poly\n",
    "            self._num_c = self.X.shape[0] \n",
    "           \n",
    "        else:\n",
    "            assert False\n",
    "            \n",
    "        self._w = torch.nn.Linear(in_features=self._num_c, out_features=1)\n",
    "\n",
    "    def rbf(self, x):\n",
    "        y = self.X.repeat(x.size(0), 1, 1)\n",
    "        return torch.exp(-self._gamma*((x[:,None]-y)**2).sum(dim=2))\n",
    "\n",
    "    def poly(self, x, c=1):\n",
    "        y = self.X.repeat(x.size(0), 1, 1)\n",
    "        return ((x@(self.X.T))+c) ** self._p     \n",
    "    \n",
    "    @staticmethod\n",
    "    def linear(x):\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self._kernel(x)\n",
    "        y = self._w(y)\n",
    "        #For Logistic we use Sigmoid\n",
    "        return torch.sigmoid(y)\n",
    "    \n",
    "    def train(self, x, y, X_val, y_val, epochs=200):\n",
    "        x = torch.FloatTensor(x)\n",
    "        y = torch.FloatTensor(y)\n",
    "        X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "        valat=epochs/10\n",
    "        optim=torch.optim.SGD(self.parameters(),lr=0.01)\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            optim.zero_grad()\n",
    "            pred=self(x)\n",
    "            loss=criterion(pred,y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            if i%valat==0:\n",
    "                ypred=self(X_val).detach()\n",
    "                ypred=ypred.transpose(1,0).reshape(y_val.shape).numpy()\n",
    "                ypred=np.asarray(ypred>=0.5,dtype=np.int64)\n",
    "                print(\"Validation Accuracy at epoch\",i,\":\",accuracy_score(y_val,ypred))\n",
    "        print(\"Training Loss is :\",loss.item())\n",
    "        \n",
    "    def test(self, x_test, y_test):\n",
    "        x_test = torch.FloatTensor(x_test)\n",
    "        ypred=self(x_test).detach()\n",
    "        ypred=ypred.transpose(1,0).reshape(y_test.shape).numpy()\n",
    "        ypred=np.asarray(ypred>=0.5,dtype=np.int64)\n",
    "        print(\"Test Accuracy:\",accuracy_score(y_test,ypred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy at epoch 0 : 0.44\n",
      "Validation Accuracy at epoch 100 : 0.56\n",
      "Validation Accuracy at epoch 200 : 0.56\n",
      "Validation Accuracy at epoch 300 : 0.56\n",
      "Validation Accuracy at epoch 400 : 0.56\n",
      "Validation Accuracy at epoch 500 : 0.56\n",
      "Validation Accuracy at epoch 600 : 0.56\n",
      "Validation Accuracy at epoch 700 : 0.56\n",
      "Validation Accuracy at epoch 800 : 0.56\n",
      "Validation Accuracy at epoch 900 : 0.56\n",
      "Training Loss is : 0.6455172300338745\n",
      "Test Accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "linLog=LogReg(X_train,kernel='linear')\n",
    "linLog.train(X_train, y_train, X_val, y_val, epochs=1000)\n",
    "linLog.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy at epoch 0 : 0.56\n",
      "Validation Accuracy at epoch 100 : 0.56\n",
      "Validation Accuracy at epoch 200 : 0.56\n",
      "Validation Accuracy at epoch 300 : 0.56\n",
      "Validation Accuracy at epoch 400 : 0.56\n",
      "Validation Accuracy at epoch 500 : 0.56\n",
      "Validation Accuracy at epoch 600 : 0.56\n",
      "Validation Accuracy at epoch 700 : 0.56\n",
      "Validation Accuracy at epoch 800 : 0.56\n",
      "Validation Accuracy at epoch 900 : 0.56\n",
      "Training Loss is : 35.230350494384766\n",
      "Test Accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "polyLog=LogReg(X_train, kernel='poly', p=10)\n",
    "polyLog.train(X_train, y_train, X_val, y_val, epochs=1000)\n",
    "polyLog.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy at epoch 0 : 0.56\n",
      "Validation Accuracy at epoch 100 : 0.56\n",
      "Validation Accuracy at epoch 200 : 0.56\n",
      "Validation Accuracy at epoch 300 : 0.56\n",
      "Validation Accuracy at epoch 400 : 0.56\n",
      "Validation Accuracy at epoch 500 : 0.56\n",
      "Validation Accuracy at epoch 600 : 0.56\n",
      "Validation Accuracy at epoch 700 : 0.56\n",
      "Validation Accuracy at epoch 800 : 0.56\n",
      "Validation Accuracy at epoch 900 : 0.56\n",
      "Training Loss is : 0.6233896017074585\n",
      "Test Accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "rbfLog=LogReg(X_train, kernel='rbf')\n",
    "rbfLog.train(X_train, y_train, X_val, y_val, epochs=1000)\n",
    "rbfLog.test(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for K=1 is 0.96\n",
      "Accuracy for K=2 is 0.96\n",
      "Accuracy for K=3 is 0.95\n",
      "Accuracy for K=4 is 0.97\n",
      "Accuracy for K=5 is 0.96\n",
      "Accuracy for K=6 is 0.97\n",
      "Accuracy for K=7 is 0.95\n",
      "Accuracy for K=8 is 0.94\n",
      "Accuracy for K=9 is 0.95\n",
      "Test Accuracy for K=6 is 0.9\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    knn = KNeighborsRegressor(n_neighbors=i, weights='distance')\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred_i = np.round(knn.predict(X_val),decimals=0)\n",
    "    mae = accuracy_score(y_val, pred_i)\n",
    "    print(f'Accuracy for K={i} is {round(mae,5)}')\n",
    "knn = KNeighborsRegressor(n_neighbors=6, weights='distance')\n",
    "knn.fit(X_train, y_train)\n",
    "pred_i = np.round(knn.predict(X_test),decimals=0)\n",
    "mae = accuracy_score(y_test, pred_i)\n",
    "print(f'Test Accuracy for K=6 is {round(mae,5)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
    "test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "def train(model, epochs):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    max_epochs = epochs\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    for epoch in range(max_epochs):\n",
    "        epochloss=0\n",
    "        val_epochloss=0\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outp = model(X_batch)\n",
    "            loss = loss_function(outp.flatten(), y_batch)\n",
    "            loss.backward()\n",
    "            epochloss = epochloss+loss.detach().flatten()[0]/len(X_batch)\n",
    "            optimizer.step()\n",
    "        losses.append(epochloss/len(train_dataloader))\n",
    "        with torch.no_grad():\n",
    "            for X_val_batch, y_val_batch in val_dataloader:\n",
    "                outp_val = model(X_val_batch).detach()\n",
    "                loss_val = loss_function(outp_val.flatten(), y_val_batch)\n",
    "                val_epochloss = val_epochloss+loss_val.detach().flatten()[0]/len(X_val_batch)\n",
    "            val_losses.append(val_epochloss/len(val_dataloader))\n",
    "        print(f\"Epoch: {epoch+1}, Train Loss: {epochloss}, Val Loss: {val_epochloss}\")\n",
    "    return model, losses, val_losses\n",
    "\n",
    "def predict(dataloader, model):\n",
    "    model.eval()\n",
    "    predictions = np.array([])\n",
    "    for x_batch, _ in dataloader:\n",
    "        outp = model(x_batch)\n",
    "        probs = torch.sigmoid(outp)\n",
    "        preds = (probs > 0.5).type(torch.long)\n",
    "        predictions = np.hstack((predictions, preds.numpy().flatten()))\n",
    "    predictions = predictions\n",
    "    return predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.2793208062648773, Val Loss: 0.23421035706996918\n",
      "Epoch: 2, Train Loss: 0.27313175797462463, Val Loss: 0.22713787853717804\n",
      "Epoch: 3, Train Loss: 0.26694124937057495, Val Loss: 0.21735507249832153\n",
      "Epoch: 4, Train Loss: 0.26197466254234314, Val Loss: 0.21129542589187622\n",
      "Epoch: 5, Train Loss: 0.26066654920578003, Val Loss: 0.21037116646766663\n",
      "Epoch: 6, Train Loss: 0.2595447599887848, Val Loss: 0.21050210297107697\n",
      "Epoch: 7, Train Loss: 0.2584787607192993, Val Loss: 0.210190549492836\n",
      "Epoch: 8, Train Loss: 0.25733107328414917, Val Loss: 0.20938587188720703\n",
      "Epoch: 9, Train Loss: 0.2559090852737427, Val Loss: 0.2084861844778061\n",
      "Epoch: 10, Train Loss: 0.2540963888168335, Val Loss: 0.20754404366016388\n",
      "Epoch: 11, Train Loss: 0.2517347037792206, Val Loss: 0.2063717544078827\n",
      "Epoch: 12, Train Loss: 0.24851948022842407, Val Loss: 0.2047843039035797\n",
      "Epoch: 13, Train Loss: 0.2439887672662735, Val Loss: 0.20258092880249023\n",
      "Epoch: 14, Train Loss: 0.23741744458675385, Val Loss: 0.1995248645544052\n",
      "Epoch: 15, Train Loss: 0.22767770290374756, Val Loss: 0.19532719254493713\n",
      "Epoch: 16, Train Loss: 0.2132866382598877, Val Loss: 0.19008928537368774\n",
      "Epoch: 17, Train Loss: 0.19567503035068512, Val Loss: 0.18626388907432556\n",
      "Epoch: 18, Train Loss: 0.17720386385917664, Val Loss: 0.18631061911582947\n",
      "Epoch: 19, Train Loss: 0.15995587408542633, Val Loss: 0.19200479984283447\n",
      "Epoch: 20, Train Loss: 0.14331118762493134, Val Loss: 0.20024308562278748\n",
      "Epoch: 21, Train Loss: 0.12646955251693726, Val Loss: 0.2059902548789978\n",
      "Epoch: 22, Train Loss: 0.1097443699836731, Val Loss: 0.20822450518608093\n",
      "Epoch: 23, Train Loss: 0.0969310998916626, Val Loss: 0.2117464244365692\n",
      "Epoch: 24, Train Loss: 0.08973323553800583, Val Loss: 0.2177513837814331\n",
      "Epoch: 25, Train Loss: 0.08588171750307083, Val Loss: 0.22521857917308807\n",
      "Epoch: 26, Train Loss: 0.08347044140100479, Val Loss: 0.23513127863407135\n",
      "Epoch: 27, Train Loss: 0.08118406683206558, Val Loss: 0.24536646902561188\n",
      "Epoch: 28, Train Loss: 0.07890370488166809, Val Loss: 0.2529294192790985\n",
      "Epoch: 29, Train Loss: 0.07707313448190689, Val Loss: 0.2569546103477478\n",
      "Epoch: 30, Train Loss: 0.07588812708854675, Val Loss: 0.25892290472984314\n",
      "Epoch: 31, Train Loss: 0.0751466229557991, Val Loss: 0.2604464590549469\n",
      "Epoch: 32, Train Loss: 0.07458173483610153, Val Loss: 0.26205670833587646\n",
      "Epoch: 33, Train Loss: 0.07405161112546921, Val Loss: 0.263572096824646\n",
      "Epoch: 34, Train Loss: 0.07353733479976654, Val Loss: 0.2647356688976288\n",
      "Epoch: 35, Train Loss: 0.07306735962629318, Val Loss: 0.26548486948013306\n",
      "Epoch: 36, Train Loss: 0.07265409827232361, Val Loss: 0.2659853994846344\n",
      "Epoch: 37, Train Loss: 0.07228396087884903, Val Loss: 0.26632386445999146\n",
      "Epoch: 38, Train Loss: 0.07194337248802185, Val Loss: 0.2665877342224121\n",
      "Epoch: 39, Train Loss: 0.07162275165319443, Val Loss: 0.266730934381485\n",
      "Epoch: 40, Train Loss: 0.07132469117641449, Val Loss: 0.2667442560195923\n",
      "Epoch: 41, Train Loss: 0.07104790210723877, Val Loss: 0.26667946577072144\n",
      "Epoch: 42, Train Loss: 0.0707872062921524, Val Loss: 0.2665448784828186\n",
      "Epoch: 43, Train Loss: 0.07054067403078079, Val Loss: 0.26636478304862976\n",
      "Epoch: 44, Train Loss: 0.07030513137578964, Val Loss: 0.26613304018974304\n",
      "Epoch: 45, Train Loss: 0.07008109241724014, Val Loss: 0.26587817072868347\n",
      "Epoch: 46, Train Loss: 0.06986459344625473, Val Loss: 0.2655763328075409\n",
      "Epoch: 47, Train Loss: 0.069658063352108, Val Loss: 0.2652392089366913\n",
      "Epoch: 48, Train Loss: 0.06946036964654922, Val Loss: 0.26486289501190186\n",
      "Epoch: 49, Train Loss: 0.06926999986171722, Val Loss: 0.2645012438297272\n",
      "Epoch: 50, Train Loss: 0.06908473372459412, Val Loss: 0.2641142010688782\n",
      "Epoch: 51, Train Loss: 0.0689060166478157, Val Loss: 0.26372602581977844\n",
      "Epoch: 52, Train Loss: 0.06873171776533127, Val Loss: 0.26330459117889404\n",
      "Epoch: 53, Train Loss: 0.06856407970190048, Val Loss: 0.26287683844566345\n",
      "Epoch: 54, Train Loss: 0.06840107589960098, Val Loss: 0.2624402940273285\n",
      "Epoch: 55, Train Loss: 0.06824228912591934, Val Loss: 0.2619967460632324\n",
      "Epoch: 56, Train Loss: 0.06808765232563019, Val Loss: 0.2615508735179901\n",
      "Epoch: 57, Train Loss: 0.06793641299009323, Val Loss: 0.26109564304351807\n",
      "Epoch: 58, Train Loss: 0.06778904795646667, Val Loss: 0.2606310248374939\n",
      "Epoch: 59, Train Loss: 0.06764551997184753, Val Loss: 0.2601703703403473\n",
      "Epoch: 60, Train Loss: 0.06750469654798508, Val Loss: 0.2597092390060425\n",
      "Epoch: 61, Train Loss: 0.06736672669649124, Val Loss: 0.2592437267303467\n",
      "Epoch: 62, Train Loss: 0.06723160296678543, Val Loss: 0.25878259539604187\n",
      "Epoch: 63, Train Loss: 0.0670986995100975, Val Loss: 0.25831305980682373\n",
      "Epoch: 64, Train Loss: 0.06696883589029312, Val Loss: 0.25784850120544434\n",
      "Epoch: 65, Train Loss: 0.06684109568595886, Val Loss: 0.25738197565078735\n",
      "Epoch: 66, Train Loss: 0.06671575456857681, Val Loss: 0.2569179832935333\n",
      "Epoch: 67, Train Loss: 0.06659254431724548, Val Loss: 0.2564556300640106\n",
      "Epoch: 68, Train Loss: 0.06647124886512756, Val Loss: 0.25599703192710876\n",
      "Epoch: 69, Train Loss: 0.06635182350873947, Val Loss: 0.25554096698760986\n",
      "Epoch: 70, Train Loss: 0.06623393297195435, Val Loss: 0.255083292722702\n",
      "Epoch: 71, Train Loss: 0.0661175549030304, Val Loss: 0.2546237111091614\n",
      "Epoch: 72, Train Loss: 0.06600348651409149, Val Loss: 0.2541712820529938\n",
      "Epoch: 73, Train Loss: 0.0658910796046257, Val Loss: 0.2537243068218231\n",
      "Epoch: 74, Train Loss: 0.06577998399734497, Val Loss: 0.2532742917537689\n",
      "Epoch: 75, Train Loss: 0.06567099690437317, Val Loss: 0.25283747911453247\n",
      "Epoch: 76, Train Loss: 0.06556244939565659, Val Loss: 0.25239133834838867\n",
      "Epoch: 77, Train Loss: 0.06545617431402206, Val Loss: 0.25195395946502686\n",
      "Epoch: 78, Train Loss: 0.06535083055496216, Val Loss: 0.2515225112438202\n",
      "Epoch: 79, Train Loss: 0.06524647027254105, Val Loss: 0.2510913908481598\n",
      "Epoch: 80, Train Loss: 0.0651433989405632, Val Loss: 0.2506676912307739\n",
      "Epoch: 81, Train Loss: 0.06504114717245102, Val Loss: 0.25024235248565674\n",
      "Epoch: 82, Train Loss: 0.06494022905826569, Val Loss: 0.24981734156608582\n",
      "Epoch: 83, Train Loss: 0.06484077870845795, Val Loss: 0.24939018487930298\n",
      "Epoch: 84, Train Loss: 0.06474300473928452, Val Loss: 0.2489837408065796\n",
      "Epoch: 85, Train Loss: 0.06464513391256332, Val Loss: 0.2485782653093338\n",
      "Epoch: 86, Train Loss: 0.06454785168170929, Val Loss: 0.24817170202732086\n",
      "Epoch: 87, Train Loss: 0.06445185095071793, Val Loss: 0.24776619672775269\n",
      "Epoch: 88, Train Loss: 0.06435690820217133, Val Loss: 0.2473667860031128\n",
      "Epoch: 89, Train Loss: 0.06426273286342621, Val Loss: 0.24697110056877136\n",
      "Epoch: 90, Train Loss: 0.06416938453912735, Val Loss: 0.2465762048959732\n",
      "Epoch: 91, Train Loss: 0.06407707184553146, Val Loss: 0.24618862569332123\n",
      "Epoch: 92, Train Loss: 0.06398504972457886, Val Loss: 0.2458009421825409\n",
      "Epoch: 93, Train Loss: 0.0638941079378128, Val Loss: 0.24541470408439636\n",
      "Epoch: 94, Train Loss: 0.06380391120910645, Val Loss: 0.24503499269485474\n",
      "Epoch: 95, Train Loss: 0.06371452659368515, Val Loss: 0.2446621060371399\n",
      "Epoch: 96, Train Loss: 0.06362565606832504, Val Loss: 0.24428826570510864\n",
      "Epoch: 97, Train Loss: 0.06353748589754105, Val Loss: 0.24391891062259674\n",
      "Epoch: 98, Train Loss: 0.06345000118017197, Val Loss: 0.24355459213256836\n",
      "Epoch: 99, Train Loss: 0.06336302310228348, Val Loss: 0.2431926429271698\n",
      "Epoch: 100, Train Loss: 0.0632767528295517, Val Loss: 0.2428302764892578\n",
      "Epoch: 101, Train Loss: 0.06319119036197662, Val Loss: 0.24247398972511292\n",
      "Epoch: 102, Train Loss: 0.06310605257749557, Val Loss: 0.24211789667606354\n",
      "Epoch: 103, Train Loss: 0.063021719455719, Val Loss: 0.24176764488220215\n",
      "Epoch: 104, Train Loss: 0.0629376545548439, Val Loss: 0.24142023921012878\n",
      "Epoch: 105, Train Loss: 0.06285423785448074, Val Loss: 0.24107088148593903\n",
      "Epoch: 106, Train Loss: 0.06277163326740265, Val Loss: 0.24073538184165955\n",
      "Epoch: 107, Train Loss: 0.06268902122974396, Val Loss: 0.24039322137832642\n",
      "Epoch: 108, Train Loss: 0.06260695308446884, Val Loss: 0.2400544285774231\n",
      "Epoch: 109, Train Loss: 0.0625259056687355, Val Loss: 0.23972058296203613\n",
      "Epoch: 110, Train Loss: 0.062445152550935745, Val Loss: 0.23938436806201935\n",
      "Epoch: 111, Train Loss: 0.062365155667066574, Val Loss: 0.23906157910823822\n",
      "Epoch: 112, Train Loss: 0.06228470429778099, Val Loss: 0.2387329339981079\n",
      "Epoch: 113, Train Loss: 0.06220521405339241, Val Loss: 0.23840920627117157\n",
      "Epoch: 114, Train Loss: 0.062126222997903824, Val Loss: 0.23809023201465607\n",
      "Epoch: 115, Train Loss: 0.06204738840460777, Val Loss: 0.23777204751968384\n",
      "Epoch: 116, Train Loss: 0.06196898967027664, Val Loss: 0.2374541163444519\n",
      "Epoch: 117, Train Loss: 0.061891235411167145, Val Loss: 0.23712818324565887\n",
      "Epoch: 118, Train Loss: 0.0618148148059845, Val Loss: 0.23682434856891632\n",
      "Epoch: 119, Train Loss: 0.06173745542764664, Val Loss: 0.23651544749736786\n",
      "Epoch: 120, Train Loss: 0.06166064739227295, Val Loss: 0.23620933294296265\n",
      "Epoch: 121, Train Loss: 0.061584219336509705, Val Loss: 0.23590399324893951\n",
      "Epoch: 122, Train Loss: 0.061508335173130035, Val Loss: 0.23560267686843872\n",
      "Epoch: 123, Train Loss: 0.06143271178007126, Val Loss: 0.23530197143554688\n",
      "Epoch: 124, Train Loss: 0.061357542872428894, Val Loss: 0.23500442504882812\n",
      "Epoch: 125, Train Loss: 0.06128276512026787, Val Loss: 0.23470883071422577\n",
      "Epoch: 126, Train Loss: 0.06120859086513519, Val Loss: 0.23440825939178467\n",
      "Epoch: 127, Train Loss: 0.061135198920965195, Val Loss: 0.2341284453868866\n",
      "Epoch: 128, Train Loss: 0.06106080114841461, Val Loss: 0.23384156823158264\n",
      "Epoch: 129, Train Loss: 0.06098698079586029, Val Loss: 0.23355433344841003\n",
      "Epoch: 130, Train Loss: 0.06091386079788208, Val Loss: 0.2332714945077896\n",
      "Epoch: 131, Train Loss: 0.06084105744957924, Val Loss: 0.23299188911914825\n",
      "Epoch: 132, Train Loss: 0.060768406838178635, Val Loss: 0.23271209001541138\n",
      "Epoch: 133, Train Loss: 0.06069609522819519, Val Loss: 0.23243333399295807\n",
      "Epoch: 134, Train Loss: 0.0606241375207901, Val Loss: 0.23215484619140625\n",
      "Epoch: 135, Train Loss: 0.06055281683802605, Val Loss: 0.23188254237174988\n",
      "Epoch: 136, Train Loss: 0.060481566935777664, Val Loss: 0.23161250352859497\n",
      "Epoch: 137, Train Loss: 0.06041046604514122, Val Loss: 0.2313292920589447\n",
      "Epoch: 138, Train Loss: 0.06034081429243088, Val Loss: 0.231073796749115\n",
      "Epoch: 139, Train Loss: 0.06026976928114891, Val Loss: 0.23080964386463165\n",
      "Epoch: 140, Train Loss: 0.06019921600818634, Val Loss: 0.23054541647434235\n",
      "Epoch: 141, Train Loss: 0.06012929975986481, Val Loss: 0.2302827388048172\n",
      "Epoch: 142, Train Loss: 0.06005983427166939, Val Loss: 0.23002350330352783\n",
      "Epoch: 143, Train Loss: 0.05999068170785904, Val Loss: 0.22976787388324738\n",
      "Epoch: 144, Train Loss: 0.05992136150598526, Val Loss: 0.22950829565525055\n",
      "Epoch: 145, Train Loss: 0.05985255166888237, Val Loss: 0.22925126552581787\n",
      "Epoch: 146, Train Loss: 0.059784289449453354, Val Loss: 0.22899866104125977\n",
      "Epoch: 147, Train Loss: 0.05971619114279747, Val Loss: 0.22874820232391357\n",
      "Epoch: 148, Train Loss: 0.05964832007884979, Val Loss: 0.22849902510643005\n",
      "Epoch: 149, Train Loss: 0.05958090350031853, Val Loss: 0.22825407981872559\n",
      "Epoch: 150, Train Loss: 0.0595131553709507, Val Loss: 0.22798804938793182\n",
      "Epoch: 151, Train Loss: 0.059447720646858215, Val Loss: 0.22776095569133759\n",
      "Epoch: 152, Train Loss: 0.0593799352645874, Val Loss: 0.22751808166503906\n",
      "Epoch: 153, Train Loss: 0.05931274965405464, Val Loss: 0.22727006673812866\n",
      "Epoch: 154, Train Loss: 0.05924668163061142, Val Loss: 0.22702857851982117\n",
      "Epoch: 155, Train Loss: 0.0591810867190361, Val Loss: 0.22679269313812256\n",
      "Epoch: 156, Train Loss: 0.059115298092365265, Val Loss: 0.2265569567680359\n",
      "Epoch: 157, Train Loss: 0.05904972180724144, Val Loss: 0.22632230818271637\n",
      "Epoch: 158, Train Loss: 0.058984480798244476, Val Loss: 0.22608940303325653\n",
      "Epoch: 159, Train Loss: 0.05891939625144005, Val Loss: 0.22585554420948029\n",
      "Epoch: 160, Train Loss: 0.05885474383831024, Val Loss: 0.22562412917613983\n",
      "Epoch: 161, Train Loss: 0.05879044160246849, Val Loss: 0.22539597749710083\n",
      "Epoch: 162, Train Loss: 0.05872626602649689, Val Loss: 0.22516730427742004\n",
      "Epoch: 163, Train Loss: 0.05866261571645737, Val Loss: 0.22494220733642578\n",
      "Epoch: 164, Train Loss: 0.05859910324215889, Val Loss: 0.22471922636032104\n",
      "Epoch: 165, Train Loss: 0.058535464107990265, Val Loss: 0.2244957536458969\n",
      "Epoch: 166, Train Loss: 0.05847199261188507, Val Loss: 0.22426839172840118\n",
      "Epoch: 167, Train Loss: 0.058408744633197784, Val Loss: 0.22401940822601318\n",
      "Epoch: 168, Train Loss: 0.05834849178791046, Val Loss: 0.22382110357284546\n",
      "Epoch: 169, Train Loss: 0.058285199105739594, Val Loss: 0.22360649704933167\n",
      "Epoch: 170, Train Loss: 0.05822199955582619, Val Loss: 0.22338052093982697\n",
      "Epoch: 171, Train Loss: 0.05816015601158142, Val Loss: 0.22316008806228638\n",
      "Epoch: 172, Train Loss: 0.05809878930449486, Val Loss: 0.2229427695274353\n",
      "Epoch: 173, Train Loss: 0.05803769826889038, Val Loss: 0.22273027896881104\n",
      "Epoch: 174, Train Loss: 0.05797650292515755, Val Loss: 0.22251765429973602\n",
      "Epoch: 175, Train Loss: 0.05791548267006874, Val Loss: 0.22230492532253265\n",
      "Epoch: 176, Train Loss: 0.057854678481817245, Val Loss: 0.22209212183952332\n",
      "Epoch: 177, Train Loss: 0.05779421702027321, Val Loss: 0.2218804955482483\n",
      "Epoch: 178, Train Loss: 0.05773415416479111, Val Loss: 0.22167159616947174\n",
      "Epoch: 179, Train Loss: 0.05767417326569557, Val Loss: 0.22146064043045044\n",
      "Epoch: 180, Train Loss: 0.05761469528079033, Val Loss: 0.22125458717346191\n",
      "Epoch: 181, Train Loss: 0.05755531042814255, Val Loss: 0.2210502326488495\n",
      "Epoch: 182, Train Loss: 0.057496022433042526, Val Loss: 0.22084546089172363\n",
      "Epoch: 183, Train Loss: 0.057437025010585785, Val Loss: 0.2206384688615799\n",
      "Epoch: 184, Train Loss: 0.05737849697470665, Val Loss: 0.2204359918832779\n",
      "Epoch: 185, Train Loss: 0.05732007324695587, Val Loss: 0.22023245692253113\n",
      "Epoch: 186, Train Loss: 0.05726092308759689, Val Loss: 0.2199968695640564\n",
      "Epoch: 187, Train Loss: 0.057206686586141586, Val Loss: 0.2198309600353241\n",
      "Epoch: 188, Train Loss: 0.0571477934718132, Val Loss: 0.21963977813720703\n",
      "Epoch: 189, Train Loss: 0.05708887800574303, Val Loss: 0.21943548321723938\n",
      "Epoch: 190, Train Loss: 0.05703149363398552, Val Loss: 0.21923717856407166\n",
      "Epoch: 191, Train Loss: 0.05697477236390114, Val Loss: 0.21904417872428894\n",
      "Epoch: 192, Train Loss: 0.0569181852042675, Val Loss: 0.21885353326797485\n",
      "Epoch: 193, Train Loss: 0.05686156451702118, Val Loss: 0.21866217255592346\n",
      "Epoch: 194, Train Loss: 0.056805115193128586, Val Loss: 0.21846824884414673\n",
      "Epoch: 195, Train Loss: 0.05674946308135986, Val Loss: 0.21827998757362366\n",
      "Epoch: 196, Train Loss: 0.056693609803915024, Val Loss: 0.21808968484401703\n",
      "Epoch: 197, Train Loss: 0.05663815513253212, Val Loss: 0.21790263056755066\n",
      "Epoch: 198, Train Loss: 0.05658269301056862, Val Loss: 0.21771593391895294\n",
      "Epoch: 199, Train Loss: 0.05652732402086258, Val Loss: 0.21752779185771942\n",
      "Epoch: 200, Train Loss: 0.05647239834070206, Val Loss: 0.21733608841896057\n",
      "Epoch: 201, Train Loss: 0.056418512016534805, Val Loss: 0.21715493500232697\n",
      "Epoch: 202, Train Loss: 0.056364160031080246, Val Loss: 0.2169722616672516\n",
      "Epoch: 203, Train Loss: 0.05630970373749733, Val Loss: 0.2167849838733673\n",
      "Epoch: 204, Train Loss: 0.05625568702816963, Val Loss: 0.21659834682941437\n",
      "Epoch: 205, Train Loss: 0.056202150881290436, Val Loss: 0.21641583740711212\n",
      "Epoch: 206, Train Loss: 0.056148990988731384, Val Loss: 0.2162361890077591\n",
      "Epoch: 207, Train Loss: 0.05609564110636711, Val Loss: 0.2160530686378479\n",
      "Epoch: 208, Train Loss: 0.05604274943470955, Val Loss: 0.21587350964546204\n",
      "Epoch: 209, Train Loss: 0.05598999187350273, Val Loss: 0.21569515764713287\n",
      "Epoch: 210, Train Loss: 0.05593522638082504, Val Loss: 0.21546846628189087\n",
      "Epoch: 211, Train Loss: 0.05588792264461517, Val Loss: 0.21533286571502686\n",
      "Epoch: 212, Train Loss: 0.05583475902676582, Val Loss: 0.21516664326190948\n",
      "Epoch: 213, Train Loss: 0.055781081318855286, Val Loss: 0.21498379111289978\n",
      "Epoch: 214, Train Loss: 0.05572902411222458, Val Loss: 0.21479934453964233\n",
      "Epoch: 215, Train Loss: 0.055678367614746094, Val Loss: 0.21462826430797577\n",
      "Epoch: 216, Train Loss: 0.0556274838745594, Val Loss: 0.2144593745470047\n",
      "Epoch: 217, Train Loss: 0.05557631701231003, Val Loss: 0.21428848803043365\n",
      "Epoch: 218, Train Loss: 0.055525314062833786, Val Loss: 0.21411597728729248\n",
      "Epoch: 219, Train Loss: 0.05547471344470978, Val Loss: 0.21394315361976624\n",
      "Epoch: 220, Train Loss: 0.05542483925819397, Val Loss: 0.21377727389335632\n",
      "Epoch: 221, Train Loss: 0.055374592542648315, Val Loss: 0.21360941231250763\n",
      "Epoch: 222, Train Loss: 0.05532439425587654, Val Loss: 0.21343685686588287\n",
      "Epoch: 223, Train Loss: 0.05527479201555252, Val Loss: 0.21326734125614166\n",
      "Epoch: 224, Train Loss: 0.05522545427083969, Val Loss: 0.21310313045978546\n",
      "Epoch: 225, Train Loss: 0.05517610162496567, Val Loss: 0.21293583512306213\n",
      "Epoch: 226, Train Loss: 0.055126991122961044, Val Loss: 0.2127733677625656\n",
      "Epoch: 227, Train Loss: 0.05507781729102135, Val Loss: 0.21260929107666016\n",
      "Epoch: 228, Train Loss: 0.05502895265817642, Val Loss: 0.21244049072265625\n",
      "Epoch: 229, Train Loss: 0.054980598390102386, Val Loss: 0.21227945387363434\n",
      "Epoch: 230, Train Loss: 0.05493239313364029, Val Loss: 0.21211464703083038\n",
      "Epoch: 231, Train Loss: 0.05488455668091774, Val Loss: 0.21195586025714874\n",
      "Epoch: 232, Train Loss: 0.05483637750148773, Val Loss: 0.21179163455963135\n",
      "Epoch: 233, Train Loss: 0.054788727313280106, Val Loss: 0.21163637936115265\n",
      "Epoch: 234, Train Loss: 0.05474083498120308, Val Loss: 0.21147604286670685\n",
      "Epoch: 235, Train Loss: 0.05469318851828575, Val Loss: 0.21130937337875366\n",
      "Epoch: 236, Train Loss: 0.054646387696266174, Val Loss: 0.21114994585514069\n",
      "Epoch: 237, Train Loss: 0.05459965020418167, Val Loss: 0.21099086105823517\n",
      "Epoch: 238, Train Loss: 0.054549213498830795, Val Loss: 0.2107725292444229\n",
      "Epoch: 239, Train Loss: 0.0545087605714798, Val Loss: 0.21066243946552277\n",
      "Epoch: 240, Train Loss: 0.05446159467101097, Val Loss: 0.21052023768424988\n",
      "Epoch: 241, Train Loss: 0.05441288277506828, Val Loss: 0.21035431325435638\n",
      "Epoch: 242, Train Loss: 0.05436607822775841, Val Loss: 0.21019010245800018\n",
      "Epoch: 243, Train Loss: 0.05432053655385971, Val Loss: 0.21003501117229462\n",
      "Epoch: 244, Train Loss: 0.05427520349621773, Val Loss: 0.20988082885742188\n",
      "Epoch: 245, Train Loss: 0.05422983691096306, Val Loss: 0.20973122119903564\n",
      "Epoch: 246, Train Loss: 0.05418422445654869, Val Loss: 0.20957563817501068\n",
      "Epoch: 247, Train Loss: 0.05413892865180969, Val Loss: 0.20942404866218567\n",
      "Epoch: 248, Train Loss: 0.0540936142206192, Val Loss: 0.2092645764350891\n",
      "Epoch: 249, Train Loss: 0.05404916778206825, Val Loss: 0.20911094546318054\n",
      "Epoch: 250, Train Loss: 0.054004814475774765, Val Loss: 0.20896373689174652\n",
      "Epoch: 251, Train Loss: 0.053960032761096954, Val Loss: 0.20881043374538422\n",
      "Epoch: 252, Train Loss: 0.05391556769609451, Val Loss: 0.2086625099182129\n",
      "Epoch: 253, Train Loss: 0.053871143609285355, Val Loss: 0.2085103988647461\n",
      "Epoch: 254, Train Loss: 0.05382701754570007, Val Loss: 0.20836329460144043\n",
      "Epoch: 255, Train Loss: 0.0537826307117939, Val Loss: 0.2082131803035736\n",
      "Epoch: 256, Train Loss: 0.0537385493516922, Val Loss: 0.20805831253528595\n",
      "Epoch: 257, Train Loss: 0.05369529500603676, Val Loss: 0.2079111784696579\n",
      "Epoch: 258, Train Loss: 0.05365191400051117, Val Loss: 0.2077658772468567\n",
      "Epoch: 259, Train Loss: 0.05360831692814827, Val Loss: 0.20761580765247345\n",
      "Epoch: 260, Train Loss: 0.053564924746751785, Val Loss: 0.20746147632598877\n",
      "Epoch: 261, Train Loss: 0.05352219566702843, Val Loss: 0.20731818675994873\n",
      "Epoch: 262, Train Loss: 0.05347910895943642, Val Loss: 0.2071690559387207\n",
      "Epoch: 263, Train Loss: 0.05343630164861679, Val Loss: 0.20702330768108368\n",
      "Epoch: 264, Train Loss: 0.05339358374476433, Val Loss: 0.20687538385391235\n",
      "Epoch: 265, Train Loss: 0.05335118994116783, Val Loss: 0.20672500133514404\n",
      "Epoch: 266, Train Loss: 0.053309135138988495, Val Loss: 0.20658205449581146\n",
      "Epoch: 267, Train Loss: 0.05326692759990692, Val Loss: 0.20643630623817444\n",
      "Epoch: 268, Train Loss: 0.0532245896756649, Val Loss: 0.20629216730594635\n",
      "Epoch: 269, Train Loss: 0.053182199597358704, Val Loss: 0.2061413824558258\n",
      "Epoch: 270, Train Loss: 0.05314046889543533, Val Loss: 0.20599859952926636\n",
      "Epoch: 271, Train Loss: 0.05309354141354561, Val Loss: 0.2057776153087616\n",
      "Epoch: 272, Train Loss: 0.05306059494614601, Val Loss: 0.20570430159568787\n",
      "Epoch: 273, Train Loss: 0.05301826819777489, Val Loss: 0.20557647943496704\n",
      "Epoch: 274, Train Loss: 0.052970435470342636, Val Loss: 0.2053731381893158\n",
      "Epoch: 275, Train Loss: 0.05293309688568115, Val Loss: 0.20526009798049927\n",
      "Epoch: 276, Train Loss: 0.052892010658979416, Val Loss: 0.20512594282627106\n",
      "Epoch: 277, Train Loss: 0.05284973233938217, Val Loss: 0.20497998595237732\n",
      "Epoch: 278, Train Loss: 0.05280847102403641, Val Loss: 0.2048400491476059\n",
      "Epoch: 279, Train Loss: 0.05276746675372124, Val Loss: 0.20469669997692108\n",
      "Epoch: 280, Train Loss: 0.05272673815488815, Val Loss: 0.2045508176088333\n",
      "Epoch: 281, Train Loss: 0.052686456590890884, Val Loss: 0.20441429316997528\n",
      "Epoch: 282, Train Loss: 0.05264576897025108, Val Loss: 0.20427364110946655\n",
      "Epoch: 283, Train Loss: 0.05260510742664337, Val Loss: 0.20413511991500854\n",
      "Epoch: 284, Train Loss: 0.05256420373916626, Val Loss: 0.20398759841918945\n",
      "Epoch: 285, Train Loss: 0.05252392590045929, Val Loss: 0.2038474828004837\n",
      "Epoch: 286, Train Loss: 0.05248371139168739, Val Loss: 0.20370540022850037\n",
      "Epoch: 287, Train Loss: 0.05244388431310654, Val Loss: 0.20357000827789307\n",
      "Epoch: 288, Train Loss: 0.052403729408979416, Val Loss: 0.20342892408370972\n",
      "Epoch: 289, Train Loss: 0.05236389487981796, Val Loss: 0.20329461991786957\n",
      "Epoch: 290, Train Loss: 0.05232418328523636, Val Loss: 0.20315994322299957\n",
      "Epoch: 291, Train Loss: 0.05228399857878685, Val Loss: 0.20301668345928192\n",
      "Epoch: 292, Train Loss: 0.05224445462226868, Val Loss: 0.20287805795669556\n",
      "Epoch: 293, Train Loss: 0.0522051639854908, Val Loss: 0.20274074375629425\n",
      "Epoch: 294, Train Loss: 0.05216577649116516, Val Loss: 0.2026052623987198\n",
      "Epoch: 295, Train Loss: 0.05212600901722908, Val Loss: 0.20246155560016632\n",
      "Epoch: 296, Train Loss: 0.05208706855773926, Val Loss: 0.20232389867305756\n",
      "Epoch: 297, Train Loss: 0.052048102021217346, Val Loss: 0.20218506455421448\n",
      "Epoch: 298, Train Loss: 0.05200909450650215, Val Loss: 0.2020471841096878\n",
      "Epoch: 299, Train Loss: 0.05196994170546532, Val Loss: 0.2019065022468567\n",
      "Epoch: 300, Train Loss: 0.05193113908171654, Val Loss: 0.20177048444747925\n",
      "Epoch: 301, Train Loss: 0.05189197510480881, Val Loss: 0.20162935554981232\n",
      "Epoch: 302, Train Loss: 0.0518532358109951, Val Loss: 0.2014942765235901\n",
      "Epoch: 303, Train Loss: 0.05181414261460304, Val Loss: 0.20134708285331726\n",
      "Epoch: 304, Train Loss: 0.05177595466375351, Val Loss: 0.20121517777442932\n",
      "Epoch: 305, Train Loss: 0.05173731967806816, Val Loss: 0.20107713341712952\n",
      "Epoch: 306, Train Loss: 0.05169883370399475, Val Loss: 0.2009430080652237\n",
      "Epoch: 307, Train Loss: 0.051660098135471344, Val Loss: 0.20080287754535675\n",
      "Epoch: 308, Train Loss: 0.051621511578559875, Val Loss: 0.2006586194038391\n",
      "Epoch: 309, Train Loss: 0.05157611146569252, Val Loss: 0.2004258930683136\n",
      "Epoch: 310, Train Loss: 0.05154964327812195, Val Loss: 0.20037628710269928\n",
      "Epoch: 311, Train Loss: 0.05151140317320824, Val Loss: 0.20025406777858734\n",
      "Epoch: 312, Train Loss: 0.05147005245089531, Val Loss: 0.20010727643966675\n",
      "Epoch: 313, Train Loss: 0.05143028497695923, Val Loss: 0.1999562680721283\n",
      "Epoch: 314, Train Loss: 0.051392797380685806, Val Loss: 0.19982361793518066\n",
      "Epoch: 315, Train Loss: 0.05135522782802582, Val Loss: 0.19968833029270172\n",
      "Epoch: 316, Train Loss: 0.05131759122014046, Val Loss: 0.19955331087112427\n",
      "Epoch: 317, Train Loss: 0.05127986520528793, Val Loss: 0.19941920042037964\n",
      "Epoch: 318, Train Loss: 0.05124199390411377, Val Loss: 0.1992875337600708\n",
      "Epoch: 319, Train Loss: 0.0512038953602314, Val Loss: 0.19915024936199188\n",
      "Epoch: 320, Train Loss: 0.05116616189479828, Val Loss: 0.19901025295257568\n",
      "Epoch: 321, Train Loss: 0.051128849387168884, Val Loss: 0.1988714039325714\n",
      "Epoch: 322, Train Loss: 0.051091693341732025, Val Loss: 0.1987399458885193\n",
      "Epoch: 323, Train Loss: 0.05105394497513771, Val Loss: 0.19859737157821655\n",
      "Epoch: 324, Train Loss: 0.05101696401834488, Val Loss: 0.19846808910369873\n",
      "Epoch: 325, Train Loss: 0.050979167222976685, Val Loss: 0.19832849502563477\n",
      "Epoch: 326, Train Loss: 0.05094197764992714, Val Loss: 0.19819460809230804\n",
      "Epoch: 327, Train Loss: 0.05090463533997536, Val Loss: 0.19805189967155457\n",
      "Epoch: 328, Train Loss: 0.05086365342140198, Val Loss: 0.19786974787712097\n",
      "Epoch: 329, Train Loss: 0.05083305388689041, Val Loss: 0.19777920842170715\n",
      "Epoch: 330, Train Loss: 0.050796814262866974, Val Loss: 0.1976609081029892\n",
      "Epoch: 331, Train Loss: 0.05075756460428238, Val Loss: 0.1975102573633194\n",
      "Epoch: 332, Train Loss: 0.050720129162073135, Val Loss: 0.19737078249454498\n",
      "Epoch: 333, Train Loss: 0.0506838895380497, Val Loss: 0.1972365826368332\n",
      "Epoch: 334, Train Loss: 0.05064720660448074, Val Loss: 0.1971009522676468\n",
      "Epoch: 335, Train Loss: 0.05061054974794388, Val Loss: 0.19696572422981262\n",
      "Epoch: 336, Train Loss: 0.05057375505566597, Val Loss: 0.19683295488357544\n",
      "Epoch: 337, Train Loss: 0.05053657665848732, Val Loss: 0.19669409096240997\n",
      "Epoch: 338, Train Loss: 0.05049940198659897, Val Loss: 0.19655439257621765\n",
      "Epoch: 339, Train Loss: 0.05046258121728897, Val Loss: 0.19641557335853577\n",
      "Epoch: 340, Train Loss: 0.0504261776804924, Val Loss: 0.19628238677978516\n",
      "Epoch: 341, Train Loss: 0.0503893680870533, Val Loss: 0.19613975286483765\n",
      "Epoch: 342, Train Loss: 0.05035338178277016, Val Loss: 0.19601230323314667\n",
      "Epoch: 343, Train Loss: 0.05031650513410568, Val Loss: 0.19587406516075134\n",
      "Epoch: 344, Train Loss: 0.050279997289180756, Val Loss: 0.19574099779129028\n",
      "Epoch: 345, Train Loss: 0.05024344474077225, Val Loss: 0.19559969007968903\n",
      "Epoch: 346, Train Loss: 0.050207559019327164, Val Loss: 0.19547058641910553\n",
      "Epoch: 347, Train Loss: 0.0501709058880806, Val Loss: 0.19533032178878784\n",
      "Epoch: 348, Train Loss: 0.05018635094165802, Val Loss: 0.19431374967098236\n",
      "Epoch: 349, Train Loss: 0.05056726559996605, Val Loss: 0.19351482391357422\n",
      "Epoch: 350, Train Loss: 0.05059676989912987, Val Loss: 0.19642114639282227\n",
      "Epoch: 351, Train Loss: 0.050307780504226685, Val Loss: 0.19623152911663055\n",
      "Epoch: 352, Train Loss: 0.05002180114388466, Val Loss: 0.1948990523815155\n",
      "Epoch: 353, Train Loss: 0.04991618171334267, Val Loss: 0.1942455768585205\n",
      "Epoch: 354, Train Loss: 0.049906175583601, Val Loss: 0.19422154128551483\n",
      "Epoch: 355, Train Loss: 0.04988008737564087, Val Loss: 0.19416019320487976\n",
      "Epoch: 356, Train Loss: 0.049850448966026306, Val Loss: 0.19413909316062927\n",
      "Epoch: 357, Train Loss: 0.04980360344052315, Val Loss: 0.1939832866191864\n",
      "Epoch: 358, Train Loss: 0.04975856840610504, Val Loss: 0.1938050538301468\n",
      "Epoch: 359, Train Loss: 0.049720924347639084, Val Loss: 0.1936539262533188\n",
      "Epoch: 360, Train Loss: 0.04968671873211861, Val Loss: 0.19353555142879486\n",
      "Epoch: 361, Train Loss: 0.049651410430669785, Val Loss: 0.19340720772743225\n",
      "Epoch: 362, Train Loss: 0.049615394324064255, Val Loss: 0.19327616691589355\n",
      "Epoch: 363, Train Loss: 0.04957948997616768, Val Loss: 0.19314174354076385\n",
      "Epoch: 364, Train Loss: 0.0495438314974308, Val Loss: 0.1930093914270401\n",
      "Epoch: 365, Train Loss: 0.04950821027159691, Val Loss: 0.19286946952342987\n",
      "Epoch: 366, Train Loss: 0.04947330430150032, Val Loss: 0.1927383542060852\n",
      "Epoch: 367, Train Loss: 0.049437686800956726, Val Loss: 0.19259953498840332\n",
      "Epoch: 368, Train Loss: 0.04940268397331238, Val Loss: 0.19246450066566467\n",
      "Epoch: 369, Train Loss: 0.049368180334568024, Val Loss: 0.1923384666442871\n",
      "Epoch: 370, Train Loss: 0.049332696944475174, Val Loss: 0.19220389425754547\n",
      "Epoch: 371, Train Loss: 0.04929696023464203, Val Loss: 0.1920589655637741\n",
      "Epoch: 372, Train Loss: 0.04926251992583275, Val Loss: 0.19192735850811005\n",
      "Epoch: 373, Train Loss: 0.049227893352508545, Val Loss: 0.1917952299118042\n",
      "Epoch: 374, Train Loss: 0.04919266700744629, Val Loss: 0.19165541231632233\n",
      "Epoch: 375, Train Loss: 0.04915791004896164, Val Loss: 0.19152233004570007\n",
      "Epoch: 376, Train Loss: 0.049122825264930725, Val Loss: 0.19138093292713165\n",
      "Epoch: 377, Train Loss: 0.04908835142850876, Val Loss: 0.19125045835971832\n",
      "Epoch: 378, Train Loss: 0.049053773283958435, Val Loss: 0.1911148577928543\n",
      "Epoch: 379, Train Loss: 0.04901888594031334, Val Loss: 0.19097557663917542\n",
      "Epoch: 380, Train Loss: 0.048984430730342865, Val Loss: 0.19084665179252625\n",
      "Epoch: 381, Train Loss: 0.04894895851612091, Val Loss: 0.19070497155189514\n",
      "Epoch: 382, Train Loss: 0.0489138700067997, Val Loss: 0.1905655860900879\n",
      "Epoch: 383, Train Loss: 0.04887919872999191, Val Loss: 0.19042882323265076\n",
      "Epoch: 384, Train Loss: 0.04884406924247742, Val Loss: 0.19028225541114807\n",
      "Epoch: 385, Train Loss: 0.048809994012117386, Val Loss: 0.19014739990234375\n",
      "Epoch: 386, Train Loss: 0.04877559468150139, Val Loss: 0.19001232087612152\n",
      "Epoch: 387, Train Loss: 0.048740871250629425, Val Loss: 0.18987570703029633\n",
      "Epoch: 388, Train Loss: 0.04870600625872612, Val Loss: 0.18973444402217865\n",
      "Epoch: 389, Train Loss: 0.04867148771882057, Val Loss: 0.18960361182689667\n",
      "Epoch: 390, Train Loss: 0.0486360527575016, Val Loss: 0.18946494162082672\n",
      "Epoch: 391, Train Loss: 0.048594385385513306, Val Loss: 0.1892489492893219\n",
      "Epoch: 392, Train Loss: 0.04856887832283974, Val Loss: 0.1891767531633377\n",
      "Epoch: 393, Train Loss: 0.04853488504886627, Val Loss: 0.18905407190322876\n",
      "Epoch: 394, Train Loss: 0.04849717766046524, Val Loss: 0.18890327215194702\n",
      "Epoch: 395, Train Loss: 0.04846110939979553, Val Loss: 0.18875272572040558\n",
      "Epoch: 396, Train Loss: 0.04842708259820938, Val Loss: 0.1886226385831833\n",
      "Epoch: 397, Train Loss: 0.04839193448424339, Val Loss: 0.1884739249944687\n",
      "Epoch: 398, Train Loss: 0.048357635736465454, Val Loss: 0.18833144009113312\n",
      "Epoch: 399, Train Loss: 0.048324547708034515, Val Loss: 0.18820159137248993\n",
      "Epoch: 400, Train Loss: 0.048290107399225235, Val Loss: 0.18806375563144684\n",
      "Epoch: 401, Train Loss: 0.0482553131878376, Val Loss: 0.18792538344860077\n",
      "Epoch: 402, Train Loss: 0.04822050780057907, Val Loss: 0.18778201937675476\n",
      "Epoch: 403, Train Loss: 0.04818655177950859, Val Loss: 0.187649667263031\n",
      "Epoch: 404, Train Loss: 0.04814033582806587, Val Loss: 0.18738892674446106\n",
      "Epoch: 405, Train Loss: 0.04812159016728401, Val Loss: 0.18735870718955994\n",
      "Epoch: 406, Train Loss: 0.04808838292956352, Val Loss: 0.18724709749221802\n",
      "Epoch: 407, Train Loss: 0.04804912954568863, Val Loss: 0.18708793818950653\n",
      "Epoch: 408, Train Loss: 0.04801375791430473, Val Loss: 0.1869467794895172\n",
      "Epoch: 409, Train Loss: 0.04797934368252754, Val Loss: 0.18679551780223846\n",
      "Epoch: 410, Train Loss: 0.04794568568468094, Val Loss: 0.1866583526134491\n",
      "Epoch: 411, Train Loss: 0.0479121208190918, Val Loss: 0.18651476502418518\n",
      "Epoch: 412, Train Loss: 0.047878898680210114, Val Loss: 0.1863807737827301\n",
      "Epoch: 413, Train Loss: 0.047844916582107544, Val Loss: 0.18624374270439148\n",
      "Epoch: 414, Train Loss: 0.04780995845794678, Val Loss: 0.18610627949237823\n",
      "Epoch: 415, Train Loss: 0.04777718335390091, Val Loss: 0.18601426482200623\n",
      "Epoch: 416, Train Loss: 0.04773297160863876, Val Loss: 0.18589089810848236\n",
      "Epoch: 417, Train Loss: 0.04769287630915642, Val Loss: 0.18570148944854736\n",
      "Epoch: 418, Train Loss: 0.04765573516488075, Val Loss: 0.1855163723230362\n",
      "Epoch: 419, Train Loss: 0.04762579873204231, Val Loss: 0.18538406491279602\n",
      "Epoch: 420, Train Loss: 0.04759325087070465, Val Loss: 0.18528904020786285\n",
      "Epoch: 421, Train Loss: 0.04756084829568863, Val Loss: 0.1851462572813034\n",
      "Epoch: 422, Train Loss: 0.047523681074380875, Val Loss: 0.18497753143310547\n",
      "Epoch: 423, Train Loss: 0.04749032482504845, Val Loss: 0.18484710156917572\n",
      "Epoch: 424, Train Loss: 0.04745621606707573, Val Loss: 0.18471181392669678\n",
      "Epoch: 425, Train Loss: 0.047421518713235855, Val Loss: 0.1845446228981018\n",
      "Epoch: 426, Train Loss: 0.04739142581820488, Val Loss: 0.1844310760498047\n",
      "Epoch: 427, Train Loss: 0.04735822603106499, Val Loss: 0.18429893255233765\n",
      "Epoch: 428, Train Loss: 0.04732189700007439, Val Loss: 0.18415804207324982\n",
      "Epoch: 429, Train Loss: 0.04728565737605095, Val Loss: 0.18398183584213257\n",
      "Epoch: 430, Train Loss: 0.047253917902708054, Val Loss: 0.1838500201702118\n",
      "Epoch: 431, Train Loss: 0.04722357541322708, Val Loss: 0.18374964594841003\n",
      "Epoch: 432, Train Loss: 0.04718676209449768, Val Loss: 0.1835930496454239\n",
      "Epoch: 433, Train Loss: 0.04714912176132202, Val Loss: 0.18342959880828857\n",
      "Epoch: 434, Train Loss: 0.04711492732167244, Val Loss: 0.18328626453876495\n",
      "Epoch: 435, Train Loss: 0.04708118364214897, Val Loss: 0.1831451803445816\n",
      "Epoch: 436, Train Loss: 0.0470489002764225, Val Loss: 0.1830296367406845\n",
      "Epoch: 437, Train Loss: 0.047013670206069946, Val Loss: 0.1828741431236267\n",
      "Epoch: 438, Train Loss: 0.04697836562991142, Val Loss: 0.1827157437801361\n",
      "Epoch: 439, Train Loss: 0.046945590525865555, Val Loss: 0.18258370459079742\n",
      "Epoch: 440, Train Loss: 0.046912651509046555, Val Loss: 0.18245279788970947\n",
      "Epoch: 441, Train Loss: 0.046879108995199203, Val Loss: 0.18230904638767242\n",
      "Epoch: 442, Train Loss: 0.04684528708457947, Val Loss: 0.18216918408870697\n",
      "Epoch: 443, Train Loss: 0.04680977761745453, Val Loss: 0.182010680437088\n",
      "Epoch: 444, Train Loss: 0.04677695780992508, Val Loss: 0.18187446892261505\n",
      "Epoch: 445, Train Loss: 0.04674539715051651, Val Loss: 0.18175272643566132\n",
      "Epoch: 446, Train Loss: 0.04670754075050354, Val Loss: 0.18165889382362366\n",
      "Epoch: 447, Train Loss: 0.046674944460392, Val Loss: 0.18143504858016968\n",
      "Epoch: 448, Train Loss: 0.04663984477519989, Val Loss: 0.18127311766147614\n",
      "Epoch: 449, Train Loss: 0.04661092534661293, Val Loss: 0.18119072914123535\n",
      "Epoch: 450, Train Loss: 0.04658014327287674, Val Loss: 0.18104922771453857\n",
      "Epoch: 451, Train Loss: 0.046538691967725754, Val Loss: 0.18090394139289856\n",
      "Epoch: 452, Train Loss: 0.04651018977165222, Val Loss: 0.18072962760925293\n",
      "Epoch: 453, Train Loss: 0.046475425362586975, Val Loss: 0.18059560656547546\n",
      "Epoch: 454, Train Loss: 0.04644440487027168, Val Loss: 0.180575430393219\n",
      "Epoch: 455, Train Loss: 0.04640338569879532, Val Loss: 0.18034720420837402\n",
      "Epoch: 456, Train Loss: 0.0463729053735733, Val Loss: 0.1801469773054123\n",
      "Epoch: 457, Train Loss: 0.04632621258497238, Val Loss: 0.17986594140529633\n",
      "Epoch: 458, Train Loss: 0.04631052538752556, Val Loss: 0.17997588217258453\n",
      "Epoch: 459, Train Loss: 0.04628264904022217, Val Loss: 0.17979618906974792\n",
      "Epoch: 460, Train Loss: 0.04623894765973091, Val Loss: 0.17959800362586975\n",
      "Epoch: 461, Train Loss: 0.04620348662137985, Val Loss: 0.1795353889465332\n",
      "Epoch: 462, Train Loss: 0.046173691749572754, Val Loss: 0.1793329119682312\n",
      "Epoch: 463, Train Loss: 0.04613258317112923, Val Loss: 0.17914332449436188\n",
      "Epoch: 464, Train Loss: 0.04610341042280197, Val Loss: 0.1791316717863083\n",
      "Epoch: 465, Train Loss: 0.046069029718637466, Val Loss: 0.17893847823143005\n",
      "Epoch: 466, Train Loss: 0.04602436721324921, Val Loss: 0.17868156731128693\n",
      "Epoch: 467, Train Loss: 0.04599235951900482, Val Loss: 0.17856596410274506\n",
      "Epoch: 468, Train Loss: 0.04596973955631256, Val Loss: 0.1784406155347824\n",
      "Epoch: 469, Train Loss: 0.04593252018094063, Val Loss: 0.1783616840839386\n",
      "Epoch: 470, Train Loss: 0.0459059476852417, Val Loss: 0.17819032073020935\n",
      "Epoch: 471, Train Loss: 0.045864760875701904, Val Loss: 0.17802396416664124\n",
      "Epoch: 472, Train Loss: 0.04583808034658432, Val Loss: 0.17788727581501007\n",
      "Epoch: 473, Train Loss: 0.04580234736204147, Val Loss: 0.17783017456531525\n",
      "Epoch: 474, Train Loss: 0.04577479138970375, Val Loss: 0.17763996124267578\n",
      "Epoch: 475, Train Loss: 0.04573075845837593, Val Loss: 0.17746636271476746\n",
      "Epoch: 476, Train Loss: 0.04570724070072174, Val Loss: 0.17735038697719574\n",
      "Epoch: 477, Train Loss: 0.045669764280319214, Val Loss: 0.17729298770427704\n",
      "Epoch: 478, Train Loss: 0.04564343020319939, Val Loss: 0.17710769176483154\n",
      "Epoch: 479, Train Loss: 0.04559944570064545, Val Loss: 0.1769385188817978\n",
      "Epoch: 480, Train Loss: 0.045575231313705444, Val Loss: 0.176814004778862\n",
      "Epoch: 481, Train Loss: 0.04554012417793274, Val Loss: 0.17670902609825134\n",
      "Epoch: 482, Train Loss: 0.045506250113248825, Val Loss: 0.17664925754070282\n",
      "Epoch: 483, Train Loss: 0.04546963796019554, Val Loss: 0.1765102744102478\n",
      "Epoch: 484, Train Loss: 0.04543811455368996, Val Loss: 0.1762540489435196\n",
      "Epoch: 485, Train Loss: 0.04540085047483444, Val Loss: 0.17612597346305847\n",
      "Epoch: 486, Train Loss: 0.04537578299641609, Val Loss: 0.17609626054763794\n",
      "Epoch: 487, Train Loss: 0.0453435443341732, Val Loss: 0.17598727345466614\n",
      "Epoch: 488, Train Loss: 0.045305393636226654, Val Loss: 0.17582960426807404\n",
      "Epoch: 489, Train Loss: 0.04526880756020546, Val Loss: 0.17555929720401764\n",
      "Epoch: 490, Train Loss: 0.045237936079502106, Val Loss: 0.17542290687561035\n",
      "Epoch: 491, Train Loss: 0.045215439051389694, Val Loss: 0.17535100877285004\n",
      "Epoch: 492, Train Loss: 0.04518125578761101, Val Loss: 0.17521212995052338\n",
      "Epoch: 493, Train Loss: 0.04514795169234276, Val Loss: 0.1750868260860443\n",
      "Epoch: 494, Train Loss: 0.04511628299951553, Val Loss: 0.17494750022888184\n",
      "Epoch: 495, Train Loss: 0.045081716030836105, Val Loss: 0.17480966448783875\n",
      "Epoch: 496, Train Loss: 0.045049361884593964, Val Loss: 0.17467308044433594\n",
      "Epoch: 497, Train Loss: 0.04501977190375328, Val Loss: 0.17453700304031372\n",
      "Epoch: 498, Train Loss: 0.04498667269945145, Val Loss: 0.1744445562362671\n",
      "Epoch: 499, Train Loss: 0.04495683312416077, Val Loss: 0.17445814609527588\n",
      "Epoch: 500, Train Loss: 0.04491906613111496, Val Loss: 0.17427381873130798\n",
      "Test Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(30, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "\n",
    "model, losses, val_losses = train(model, 500)\n",
    "accuracy = accuracy_score(y_test, predict(test_dataloader, model))\n",
    "\n",
    "print(\"Test Accuracy:\",accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regulazised SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy at epoch 0 : 0.44\n",
      "Validation Accuracy at epoch 100 : 0.56\n",
      "Validation Accuracy at epoch 200 : 0.56\n",
      "Validation Accuracy at epoch 300 : 0.56\n",
      "Validation Accuracy at epoch 400 : 0.56\n",
      "Validation Accuracy at epoch 500 : 0.56\n",
      "Validation Accuracy at epoch 600 : 0.56\n",
      "Validation Accuracy at epoch 700 : 0.56\n",
      "Validation Accuracy at epoch 800 : 0.56\n",
      "Validation Accuracy at epoch 900 : 0.56\n",
      "Training Loss is : 0.7200688719749451\n",
      "Test Accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "linSVM=SVM(X, kernel='linear')\n",
    "linSVM.train(X_train, y_train, X_val, y_val, epochs=1000, lambda_reg=0.03)\n",
    "linSVM.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.7347666e-02 4.1757957e+01 1.7208124e-02 1.2365642e-02 1.3283963e-02\n",
      " 1.6796706e-02 2.8829953e-02 1.9389622e-02 1.8899956e+01 2.1611920e-02\n",
      " 2.1470062e-02]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['mean perimeter', 'mean area', 'mean smoothness', 'mean concavity',\n",
       "       'texture error', 'concavity error', 'concave points error',\n",
       "       'worst perimeter', 'worst area', 'worst concavity',\n",
       "       'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w=(linSVM._w.weight)/0.01\n",
    "w=w.detach().numpy()\n",
    "print(w[0][np.where(w>0.01)[1]])\n",
    "data.feature_names[np.where(w>0.01)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy at epoch 0 : 0.44\n",
      "Validation Accuracy at epoch 500 : 0.56\n",
      "Validation Accuracy at epoch 1000 : 0.44\n",
      "Validation Accuracy at epoch 1500 : 0.63\n",
      "Validation Accuracy at epoch 2000 : 0.56\n",
      "Validation Accuracy at epoch 2500 : 0.9\n",
      "Validation Accuracy at epoch 3000 : 0.81\n",
      "Validation Accuracy at epoch 3500 : 0.78\n",
      "Validation Accuracy at epoch 4000 : 0.79\n",
      "Validation Accuracy at epoch 4500 : 0.81\n",
      "Training Loss is : 734.1683349609375\n",
      "Test Accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "polySVM=SVM(X_train, kernel='poly', p=7)\n",
    "polySVM.train(X_train, y_train, X_val, y_val, epochs=5000, lambda_reg=0.03)\n",
    "polySVM.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy at epoch 0 : 0.56\n",
      "Validation Accuracy at epoch 100 : 0.69\n",
      "Validation Accuracy at epoch 200 : 0.77\n",
      "Validation Accuracy at epoch 300 : 0.8\n",
      "Validation Accuracy at epoch 400 : 0.82\n",
      "Validation Accuracy at epoch 500 : 0.82\n",
      "Validation Accuracy at epoch 600 : 0.82\n",
      "Validation Accuracy at epoch 700 : 0.85\n",
      "Validation Accuracy at epoch 800 : 0.85\n",
      "Validation Accuracy at epoch 900 : 0.85\n",
      "Training Loss is : 0.6085423231124878\n",
      "Test Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "rbfSVM=SVM(X_train, kernel='rbf', gamma=15)\n",
    "rbfSVM.train(X_train , y_train, X_val, y_val, epochs=1000, lambda_reg=0.03)\n",
    "rbfSVM.test(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
