{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "allchars = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',' ']\n",
    "alllang = ['e','j','s']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = []\n",
    "train_Y = []\n",
    "\n",
    "for i,lang in enumerate(alllang):\n",
    "    for N in range(10):\n",
    "        with open(f\"languageID/{lang}{N}.txt\",'r+') as f:\n",
    "            txt = f.readlines()\n",
    "        txt = ''.join(txt)\n",
    "        txt = Counter(txt)\n",
    "        txt = [txt[i] for i in allchars]\n",
    "        train_X.append(txt)\n",
    "        train_Y.append(i)\n",
    "train_X = np.asarray(train_X)\n",
    "train_Y = np.asarray(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06016851 0.01113497 0.02151    0.02197258 0.10536924 0.01893276\n",
      " 0.01747894 0.04721626 0.05541054 0.00142078 0.00373369 0.02897737\n",
      " 0.02051875 0.05792169 0.0644639  0.01675202 0.0005617  0.05382455\n",
      " 0.06618206 0.08012556 0.02666446 0.00928465 0.01549645 0.00115645\n",
      " 0.01384437 0.00062779 0.17924996]\n"
     ]
    }
   ],
   "source": [
    "all_eng = train_X[train_Y==0]\n",
    "count_eng = np.sum(all_eng,axis=0)\n",
    "count_eng = count_eng+0.5\n",
    "prob_eng = count_eng/sum(count_eng)\n",
    "print(prob_eng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.31765610e-01 1.08669066e-02 5.48586603e-03 1.72263182e-02\n",
      " 6.02047591e-02 3.87854223e-03 1.40116706e-02 3.17621161e-02\n",
      " 9.70334393e-02 2.34110207e-03 5.74094133e-02 1.43261470e-03\n",
      " 3.97987351e-02 5.67105769e-02 9.11632132e-02 8.73545547e-04\n",
      " 1.04825466e-04 4.28037318e-02 4.21747790e-02 5.69901115e-02\n",
      " 7.06174220e-02 2.44592753e-04 1.97421294e-02 3.49418219e-05\n",
      " 1.41514379e-02 7.72214263e-03 1.23449457e-01]\n"
     ]
    }
   ],
   "source": [
    "all_jap = train_X[train_Y==1]\n",
    "count_jap = np.sum(all_jap,axis=0)\n",
    "count_jap = count_jap+0.5\n",
    "prob_jap = count_jap/sum(count_jap)\n",
    "print(prob_jap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.04560451e-01 8.23286362e-03 3.75258241e-02 3.97459221e-02\n",
      " 1.13810860e-01 8.60287996e-03 7.18448398e-03 4.53270019e-03\n",
      " 4.98597021e-02 6.62945947e-03 2.77512257e-04 5.29431717e-02\n",
      " 2.58086399e-02 5.41765595e-02 7.24923684e-02 2.42669051e-02\n",
      " 7.67783910e-03 5.92951189e-02 6.57704049e-02 3.56140730e-02\n",
      " 3.37023219e-02 5.88942678e-03 9.25040856e-05 2.49761031e-03\n",
      " 7.86284728e-03 2.68261848e-03 1.68264932e-01]\n"
     ]
    }
   ],
   "source": [
    "all_spa = train_X[train_Y==2]\n",
    "count_spa = np.sum(all_spa,axis=0)\n",
    "count_spa = count_spa+0.5\n",
    "prob_spa = count_spa/sum(count_spa)\n",
    "print(prob_spa)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[164, 32, 53, 57, 311, 55, 51, 140, 140, 3, 6, 85, 64, 139, 182, 53, 3, 141, 186, 225, 65, 31, 47, 4, 38, 2, 498]\n"
     ]
    }
   ],
   "source": [
    "with open(\"languageID/e10.txt\",'r+') as f:\n",
    "    txt = f.readlines()\n",
    "    txt = ''.join(txt)\n",
    "    txt = Counter(txt)\n",
    "\n",
    "bag_e10 = [txt[i] for i in allchars]\n",
    "print(bag_e10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 and 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7841.865447060635 -8771.433079075032 -8467.282044010557\n",
      "Predicted class is 'e'\n"
     ]
    }
   ],
   "source": [
    "p_eng = sum(bag_e10*np.log(prob_eng))\n",
    "p_spa = sum(bag_e10*np.log(prob_spa))\n",
    "p_jap = sum(bag_e10*np.log(prob_jap))\n",
    "print(p_eng, p_jap, p_spa)\n",
    "print(f\"Predicted class is \\'{alllang[np.argmax([p_eng, p_jap, p_spa])]}\\'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = []\n",
    "test_Y = []\n",
    "for i,lang in enumerate(['e','j','s']):\n",
    "    for N in range(10,20):\n",
    "        with open(f\"languageID/{lang}{N}.txt\",'r+') as f:\n",
    "            txt = f.readlines()\n",
    "        txt = ''.join(txt)\n",
    "        txt = Counter(txt)\n",
    "        txt = [txt[i] for i in allchars]\n",
    "        test_X.append(txt)\n",
    "        test_Y.append(i)\n",
    "test_X = np.asarray(test_X)\n",
    "test_Y = np.asarray(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for x in test_X:\n",
    "    p_eng = sum(x*np.log(prob_eng))\n",
    "    p_spa = sum(x*np.log(prob_spa))\n",
    "    p_jap = sum(x*np.log(prob_jap))\n",
    "    y_pred.append(np.argmax([p_eng, p_jap, p_spa]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\tEng\tJap\tSpa\n",
      "Eng\t10\t0\t0\n",
      "Jap\t0\t10\t0\n",
      "Spa\t0\t0\t10\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix\")\n",
    "print(\"\\tEng\\tJap\\tSpa\")\n",
    "confmat = confusion_matrix(test_Y,y_pred)\n",
    "print(f\"Eng\\t{confmat[0][0]}\\t{confmat[0][1]}\\t{confmat[0][2]}\")\n",
    "print(f\"Jap\\t{confmat[1][0]}\\t{confmat[1][1]}\\t{confmat[1][2]}\")\n",
    "print(f\"Spa\\t{confmat[2][0]}\\t{confmat[2][1]}\\t{confmat[2][2]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Original Text===\n",
      "except when the winds rise to a high speed we seem to live in a very tranquil world at night when the glare of the sun passes out of our atmosphere the stars and planets seem to move across the heavens with a stately and solemn slowness it was one of the first discoveries of modern astronomy that this movement is only apparent the apparent creeping of the stars across the heavens at night is accounted for by the fact that the earth turns upon its axis once in every twentyfour hours when we remember the size of the earth we see that this implies a prodigious speed\n",
      "\n",
      "fig the milky way\n",
      "\n",
      "\n",
      "\n",
      "it had remained unchanged since noon of the previous daya long low quietlooking cloud not very dense or brilliant or in any way remarkable except for its size at  pm the professor left the spectroscope for a short time and on returning half an hour later to his observations he was astonished to find the gigantic sun flame shattered to pieces the solar atmosphere was filled with flying debris and some of these portions reached a height of  miles above the solar surface moving with a velocity which even at the distance of  miles was almost perceptible to the eye these fragments doubled their height in ten minutes on january   another distinguished solar observer the late professor tacchini of rome observed one of the greatest prominences ever seen by man its height was no less than  mileseighteen times the diameter of the earth another mighty flame was so vast that supposing the eight large planets of the solar system ranged one on top of the other the prominence would still tower above them\n",
      "\n",
      "here let us return to and see what more we know about the photospherethe suns surface it is from the photosphere that we have gained most of our knowledge of the composition of the sun which is believed not to be a solid body examination of the photosphere shows that the outer surface is never at rest small bright cloudlets come and go in rapid succession giving the surface through contrasts in luminosity a granular appearance of course to be visible at all at  miles the cloudlets cannot be small they imply enormous activity in the photosphere if we might speak picturesquely the suns surface resembles a boiling ocean of whitehot metal vapours we have today a wonderful instrument which will be described later which dilutes as it were the general glare of the sun and enables us to observe these fiery eruptions at any hour the oceans of redhot gas and whitehot metal vapour at the suns surface are constantly driven by great storms some unimaginable energy streams out from the body or muscles of the sun and blows its outer layers into gigantic shreds as it werepg \n",
      "\n",
      "the great sunspot of july  \n",
      "\n",
      "showing the rings mighty swarms of meteorites\n",
      "\n",
      "\n",
      "\n",
      " drawing by prof lowell made january  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "===Shuffeled Text===\n",
      "r esloabe oeeefli   tea osn ee ohathe ag p ec  ayetw iesgvuwhet a\n",
      "tsi  iti tdrgcwaeifhobsdrrwt sene v rce tpksrrn emtiapm eu settodeoeh ygnhusgld lmout  l emt sesrafc geannat pahoeia tltla  hgtpilo  nh t herpts oyssy  o ee emdti\n",
      "htfeh  ootrnts  xndyeet t  pr e  eh un mr tfa  ms rnsr  neasa s  iwasoilln sothm  alo hat rsthr wt srir pum ra \n",
      "iorlwtbllbt  p\n",
      "  h tvuo  o et t reeosiaa h   e r etvn atnsihhtlheoapatesi i rift kshwe  jeeesnemoberc vsew n   novan ed eowgtesecdsim i oeoaeoeoyrtshsgxt adlaesrfa  mpee tccternhr bsf yutregrt upaeohlo tluui  h ih irmtascnup s o  ym tawfss aahineptptdrotw iae  sobatoamhifvaphy eoes shinittiryh reep  ei eois dn  sct ma ghtejitt    alpwhoe ucsterleornn ihaeo sknh mnltueta rmt ti   tensaomdhnairm rhttpseetet otogolsd  aohe  gal hr s nngclas uh ir wraleehoui  hhaecietheohoitetwtui po sg\n",
      "afoesls euihurthhtr aogrleh nle aaunsfrss tec  tc sshtlpg n tdbfac reptcereycu  rnlsnyrsntmayoimbameu\n",
      "dtls r srd  e i  we aw n fe\n",
      "law \n",
      "neh e o el eisbnd oneeseaouegan seiererirotys tgeem  e    ire ism f phfeegn t fimttests ycnlutivrtoehoroecrhh  misleooitsrmfa irgei stteolcssee  v erebjtfwomg kvget ecivfa aaeaidl toebeufusheielneteet iwo df vnlrhl deslz aoihhundeh hhbvnegeenir sy eaol sfos  ue  e iutdeolelswrotl dfltahoe ase rpv bssiou  c ttostw n fcowhrt \n",
      "ortwaorlwo sammee mofeusiiyiiiattrentrrofa \n",
      "seu qhn eb vi iaalbespol oftehuae ln tnapeoiirsno ta  e ie t ssnhem e ongn ueuhvi ayp sbnrs\n",
      "t i  lrif ucenetmde lsoo  areiwts olorhnirgssiptwn  i s uob if lh\n",
      "nf src onrshavegsamg  o oioafeeeaespntoh  u gh aecrtl  srleeom t   tlttodf rttcmsb s ah eesealhnw\n",
      "  a s arcea nsthsdtsee ohiosfnhseesehts fbduitm nh  pi enlrle i te t tah aesllan ryela eoivti homnceng e t tg reaeeo nlcwnhnofrnganoseyd  ecpetgdogmeiooliii nehctsttmmtre s  tnuroattl hmhiopweo r e niemrepmu ooi es tseihritnemowvsrnete eyltolvgtamh on sgdratcaleamtrithdei nsaueaegasci a vmsdpgegth bfsasenrssguedg drerrta  ow oo is i chfhsheeosde fp eapneaesu  ra eeh\n",
      "ief\n",
      " u  aam oatnees hile see otpss  nmtxbfnehdopzwmm\n",
      " s tlbretsegshottmdheh sr o tonru nya rpht  sosrioeoicentd psr ueneaeepobdufosy octrstt tu r yry sspy ovtavtryohe hnwnlwiyuuiti  hafbtaf  eaeh dbtrn\n",
      "sthhitgttee o rlrs honufwntayrqf issflnfeotuporeoi olrtrstoergun ad frteo s  slttpyoe tn oissgsna ioo pb ptadm i seiaih ehcsdes tresnwesagy eue hsthh uwr ha ehnndrcruwechs asttee n eamvouheoonee iitntnsindatlsti  rorecet  o lhfepmmcnoe redrfnfoot ivt ogtedoew n o hahim hheco    hidhkob ihop iaitc roednyiemad ees htpuvlto nfuaaeonxsdmoritntnht\n",
      "h dnelchefnhersl ktc b  man h\n",
      "tnieeahuaseageshr ea a\n",
      "nsw gv c  tobpapdieoatubie ia yat ne  eno pisodgt l  eyo    e wtsuti ehtrd nahsai\n",
      "o arpo  n e soi aot\n",
      "yysa v vvh oatee her gsstbwtcoont waeaa  snmreh oenlom edcnwcn tao yrw eertha us sico tq oaa twee el n adrnian n vsyet aroanrloiuhc of a\n",
      "\n",
      "====================\n",
      "\n",
      "-7841.865447060635 -8771.433079075032 -8467.282044010557\n",
      "Predicted class is 'e'\n"
     ]
    }
   ],
   "source": [
    "with open(\"languageID/e10.txt\",'r+') as f:\n",
    "    txt = f.readlines()\n",
    "    txt = ''.join(txt)\n",
    "    print(\"===Original Text===\")\n",
    "    print(txt)\n",
    "    temp = list(txt)\n",
    "    random.shuffle(temp)\n",
    "    txt = ''.join(temp)\n",
    "    print(\"===Shuffeled Text===\")\n",
    "    print(txt)\n",
    "    txt = Counter(txt)\n",
    "print(\"\\n====================\\n\")\n",
    "bag_e10 = [txt[i] for i in allchars]\n",
    "\n",
    "p_eng = sum(bag_e10*np.log(prob_eng))\n",
    "p_spa = sum(bag_e10*np.log(prob_spa))\n",
    "p_jap = sum(bag_e10*np.log(prob_jap))\n",
    "print(p_eng, p_jap, p_spa)\n",
    "print(f\"Predicted class is \\'{alllang[np.argmax([p_eng, p_jap, p_spa])]}\\'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
